{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.read_excel('G:\\\\Pricing\\\\PPA\\\\Iberostar\\\\query1.xlsx') #import the data from the shared drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MERCHANTID', 'ORDERID', 'EFFORTID', 'ATTEMPTID', 'MOEA',\n",
       "       'CURRENCYCODE', 'Payment product ID', 'Payment product name',\n",
       "       'Acquirer', 'EMAILADDRESS', 'SERVICEPROVIDERID', 'BIN', 'Debit/Credit',\n",
       "       'Consumer/Commercial', 'Classic/Platinum', 'Bin Country', 'Status ID',\n",
       "       'Status descriptor', 'Authorized/ Not Authorized', 'Response ID',\n",
       "       'Response descriptor', 'RTA', 'MES', 'RECEIVEDDATE', 'STATUSDATE',\n",
       "       'MYAMOUNT', 'COUNT(*)', 'Pre/Post Payment', 'ER', 'Amount in EUR'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 643,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=df.copy() #create reusable copy\n",
    "df1.columns #review which columns are within the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the key\n",
    "KEY = df1['MERCHANTID'].astype(str) + df1['CURRENCYCODE'].astype(str) + df1['MYAMOUNT'].astype(str) + df1['EMAILADDRESS'].astype(str)\n",
    "df1['KEY']=KEY\n",
    "df1['KEY_SUBGROUP']=0\n",
    "df1['SUBGROUP_LIMIT']=0\n",
    "df1['THRESHOLD']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data=[]\n",
    "unique_keys=df1['KEY'].drop_duplicates().reset_index(drop=True)\n",
    "grp=df1[['ORDERID','KEY','MYAMOUNT','RECEIVEDDATE','KEY_SUBGROUP','SUBGROUP_LIMIT','THRESHOLD']][(~df1['KEY'].str.contains('nan'))].copy().reset_index(drop=True) #create the data cluster we're going to make our comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n"
     ]
    }
   ],
   "source": [
    "for x in range(0,len(unique_keys)):\n",
    "\n",
    "    grp_dts=grp[(grp['KEY']==unique_keys.iloc[x])] #evaluate the keys\n",
    "    grp_dts=grp_dts.sort_values(['KEY','RECEIVEDDATE']).reset_index(drop=True).copy() #sort the values by received date and reset the index\n",
    "\n",
    "    try:\n",
    "        for y in range(0,len(grp_dts)):\n",
    "            dt=grp_dts['RECEIVEDDATE'].iloc[0]\n",
    "            threshold=dt+timedelta(seconds=3600) #this sets the threshold of what cannot be exceeded\n",
    "            grp_dts['THRESHOLD'][(grp_dts['RECEIVEDDATE']<=threshold)]=threshold #this applies the threshold against everydate\n",
    "            grp_dts['KEY_SUBGROUP'][(grp_dts['THRESHOLD']!=0)]=y #this applies a grouping    \n",
    "            grp_dts_upload=grp_dts[(grp_dts['THRESHOLD']!=0)].reset_index(drop=True).copy() #this isolates all instances where there is a threshold \n",
    "\n",
    "            for w in range(0,len(grp_dts_upload)):\n",
    "                grouped_data.append([grp_dts_upload['ORDERID'].iloc[w],grp_dts_upload['KEY'].iloc[w],grp_dts_upload['MYAMOUNT'].iloc[w],grp_dts_upload['RECEIVEDDATE'].iloc[w],grp_dts_upload['THRESHOLD'].iloc[w],grp_dts_upload['KEY_SUBGROUP'].iloc[w]]) #appends the upload to the data array\n",
    "            \n",
    "            grp_dts=grp_dts[(grp_dts['THRESHOLD']==0)].reset_index(drop=True).copy()#this then says that the previous parent child relationship has been removed and now the code can be applied to the next\n",
    "            \n",
    "            clear_output() #clear iteration status percentage\n",
    "    \n",
    "    except:\n",
    "        for y in range(0,len(grp_dts)):\n",
    "            dt=grp_dts['RECEIVEDDATE'].iloc[0]\n",
    "            threshold=dt+timedelta(seconds=3600) #this sets the threshold of what cannot be exceeded\n",
    "            grp_dts['THRESHOLD'][(grp_dts['RECEIVEDDATE']<=threshold)]=threshold #this applies the threshold against everydate\n",
    "            grp_dts['KEY_SUBGROUP'][(grp_dts['THRESHOLD']!=0)]=y #this applies a grouping    \n",
    "            grp_dts_upload=grp_dts[(grp_dts['THRESHOLD']!=0)].reset_index(drop=True).copy() #this isolates all instances where there is a threshold \n",
    "            \n",
    "            for w in range(0,len(grp_dts_upload)):\n",
    "                grouped_data.append([grp_dts_upload['ORDERID'].iloc[w],grp_dts_upload['KEY'].iloc[w],grp_dts_upload['MYAMOUNT'].iloc[w],grp_dts_upload['RECEIVEDDATE'].iloc[w],grp_dts_upload['THRESHOLD'].iloc[w],grp_dts_upload['KEY_SUBGROUP'].iloc[w]]) #appends the upload to the data array\n",
    "            \n",
    "            grp_dts=grp_dts[(grp_dts['THRESHOLD']==0)].reset_index(drop=True).copy()#this then says that the previous parent child relationship has been removed and now the code can be applied to the next\n",
    "            \n",
    "            clear_output() #clear iteration status percentage\n",
    "    \n",
    "    clear_output()\n",
    "    print(round((x/len(unique_keys))*100,2)) #show iteration status percentage\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup the data as a dataframe        \n",
    "grpd=pd.DataFrame(grouped_data)\n",
    "#name the columns of the newly created dataframe accordingly\n",
    "grpd=grpd.rename(columns={0:'ORDERID',1:'KEY',2:'MYAMOUNT',3:'RECEIVEDDATE',4:'THRESHOLD',5:'KEY_SUBGROUP'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a grouped key\n",
    "grpd['KEY_SUBGROUP']=grpd['KEY_SUBGROUP'].astype(str) #the keygroups are numbers initially this sets them to be strings in order that they be concatenated with the email addresses\n",
    "grpd['GROUPED_KEY']=grpd['KEY']+grpd['KEY_SUBGROUP'] #concatenation procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "grpd=grpd[['ORDERID','KEY','MYAMOUNT','RECEIVEDDATE','GROUPED_KEY']].copy() #select on those data elements we want to see\n",
    "df2=pd.merge(df1,grpd,on=['ORDERID','KEY','MYAMOUNT','RECEIVEDDATE'],how='left') #merge the subgroup key data with the initial data to create a master dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n"
     ]
    }
   ],
   "source": [
    "#3- Build a function that allows for allow of this to fit together\n",
    "#create a set of unique values\n",
    "unique_grouped_keys=df2['GROUPED_KEY'].drop_duplicates().reset_index(drop=True)\n",
    "data=[]\n",
    "\n",
    "for x in range(0,len(unique_grouped_keys)):\n",
    "    grp=df2[(df2['GROUPED_KEY']==unique_grouped_keys.iloc[x])] #evaluate the keys\n",
    "    grp=grp.sort_values(['KEY','RECEIVEDDATE']).reset_index(drop=True).copy()\n",
    "    \n",
    "    #test_min_dt=test_group[['RECEIVEDDATE']].min() #identify the min date\n",
    "    #test_min_dt=test_min_dt.iloc[0] #convert min date to a series\n",
    "    #test_min_dt_threshold=test_min_dt+timedelta(seconds=3600) #update the date series to be an hour more than the min\n",
    "    \n",
    "    max_date=grp['RECEIVEDDATE'].max()\n",
    "    grp['FINALTRANSACTION']=max_date\n",
    "    grp['FINALTRANSACTION_EVALUATION']=grp['RECEIVEDDATE']==grp['FINALTRANSACTION']\n",
    "    \n",
    "    for y in range(0,len(grp['RECEIVEDDATE'])):\n",
    "        data.append([grp['MERCHANTID'].iloc[y],\n",
    "                     grp['ORDERID'].iloc[y],\n",
    "                     grp['EFFORTID'].iloc[y],\n",
    "                     grp['ATTEMPTID'].iloc[y],\n",
    "                     grp['MOEA'].iloc[y],\n",
    "                     grp['CURRENCYCODE'].iloc[y],\n",
    "                     grp['Payment product ID'].iloc[y],\n",
    "                     grp['Payment product name'].iloc[y],\n",
    "                     grp['Acquirer'].iloc[y],\n",
    "                     grp['EMAILADDRESS'].iloc[y],\n",
    "                     grp['RECEIVEDDATE'].iloc[y],\n",
    "                     grp['MYAMOUNT'].iloc[y],\n",
    "                     grp['KEY'].iloc[y],\n",
    "                     grp['GROUPED_KEY'].iloc[y],\n",
    "                     grp['FINALTRANSACTION'].iloc[y],\n",
    "                     grp['FINALTRANSACTION_EVALUATION'].iloc[y]])\n",
    "    clear_output() #clear iteration status percentage\n",
    "    print(round((x/len(unique_grouped_keys))*100,2)) #show iteration status percentage\n",
    "\n",
    "#convert the series data into a dataframe\n",
    "df3=pd.DataFrame(data)\n",
    "#name the columns of the newly created dataframe accordingly\n",
    "df3=df3.rename(columns={0:'MERCHANTID',1:'ORDERID',2:'EFFORTID',3:'ATTEMPTID',4:'MOEA'\n",
    "                        ,5:'CURRENCYCODE',6:'Payment product ID',7:'Payment product name'\n",
    "                        ,8:'Acquirer',9:'EMAILADDRESS',10:'RECEIVEDDATE',11:'MYAMOUNT',12:'KEY'\n",
    "                        ,13:'GROUPED_KEY',14:'FINALTRANSACTION',15:'FINALTRANSACTION_EVALUATION'})\n",
    "#only bring back those instances where the final transaction evaluation is true\n",
    "final_trxn=df3[(df3['FINALTRANSACTION_EVALUATION']==True)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the final transaction information to the original data set\n",
    "output=pd.merge(df2,final_trxn,how='left',on=['MERCHANTID','ORDERID','EFFORTID'\n",
    "                                             ,'ATTEMPTID','MOEA','CURRENCYCODE'\n",
    "                                             ,'Payment product ID','Payment product name'\n",
    "                                             ,'Acquirer','EMAILADDRESS','RECEIVEDDATE','MYAMOUNT'\n",
    "                                             ,'KEY','GROUPED_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\djohnson\\new_python_version\\Local\\Continuum\\anaconda2\\envs\\py3k\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "output['FINALTRANSACTION_EVALUATION'][(output['FINALTRANSACTION_EVALUATION']!=True)]=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('d:\\djohnson\\Desktop\\iberostar_final_auth.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
